{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b1efa-c015-41b6-a01f-65cdfc9b12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate paragraph.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a51ac-3319-4561-8532-5f6992b880c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def is_english_regex(s):\n",
    "    return bool(re.match('^[a-zA-Z]+$', s))\n",
    "\n",
    "total_lines_processed = 0\n",
    "empty_after_filter_count = 0\n",
    "all_processed_paragraphs = []\n",
    "\n",
    "for i in range(7):\n",
    "    file_path = f'data/origin/filter-22-23_{i}.csv'\n",
    "    print(f\"正在处理文件: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                cleaned_line = line.strip()\n",
    "                \n",
    "                if not cleaned_line:\n",
    "                    continue\n",
    "                \n",
    "                total_lines_processed += 1\n",
    "                seg_list = cleaned_line.split()\n",
    "                \n",
    "                result = [w for w in seg_list if not is_english_regex(w)]\n",
    "                \n",
    "                if not result:\n",
    "                    empty_after_filter_count += 1\n",
    "                    continue\n",
    "                \n",
    "                all_processed_paragraphs.append(result)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"警告：文件 {file_path} 未找到，已跳过。\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n--- 所有文件处理完成，开始去重和统计 ---\")\n",
    "\n",
    "count_before_dedup = len(all_processed_paragraphs)\n",
    "\n",
    "unique_paragraphs_set = {tuple(p) for p in all_processed_paragraphs}\n",
    "\n",
    "final_unique_paragraphs = [list(p) for p in unique_paragraphs_set]\n",
    "\n",
    "count_after_dedup = len(final_unique_paragraphs)\n",
    "\n",
    "print(\"\\n--- 统计结果 ---\")\n",
    "print(f\"处理的原始总行数 (非空): {total_lines_processed}\")\n",
    "print(f\"因过滤英文后变为空行而被移除的数量: {empty_after_filter_count}\")\n",
    "print(f\"去重前的有效段落总数: {count_before_dedup}\")\n",
    "print(f\"移除的重复段落数: {count_before_dedup - count_after_dedup}\")\n",
    "print(f\"去重后的最终独立段落数: {count_after_dedup}\")\n",
    "\n",
    "\n",
    "\n",
    "output_dir = 'data/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "output_pickle_path = os.path.join(output_dir, 'paragraph.pkl')\n",
    "\n",
    "print(f\"\\n正在将 {count_after_dedup} 个独立段落保存为pickle文件: {output_pickle_path}\")\n",
    "with open(output_pickle_path, 'wb') as f: \n",
    "    pickle.dump(final_unique_paragraphs, f)\n",
    "\n",
    "print(\"保存完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0db639-8bcd-4443-b0ea-a93e3f802aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract doc_vectors of paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1c4d9-0de9-431c-80bb-27c61ef0a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "INPUT_PICKLE_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_VECTORS_PATH = 'data/doc_vectors.npy'\n",
    "\n",
    "MODEL_ID = 'BAAI/bge-large-zh-v1.5'\n",
    "\n",
    "def extract_document_vectors():\n",
    "    output_dir = os.path.dirname(OUTPUT_VECTORS_PATH)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(INPUT_PICKLE_PATH):\n",
    "        raise FileNotFoundError(f\"输入文件未找到，请检查路径: {INPUT_PICKLE_PATH}\")\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_PICKLE_PATH} 加载预分词的文本... ---\")\n",
    "    with open(INPUT_PICKLE_PATH, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"--- 成功加载 {len(processed_texts)} 篇文档。---\")\n",
    "\n",
    "    print(\"\\n--- 正在将分词列表拼接成无空格的字符串... ---\")\n",
    "    pseudo_raw_documents = [\"\".join(doc) for doc in processed_texts]\n",
    "    print(f\"--- 成功准备 {len(pseudo_raw_documents)} 个字符串用于编码。---\")\n",
    "\n",
    "    if pseudo_raw_documents:\n",
    "        print(f\"处理后示例文本: '{pseudo_raw_documents[0][:70]}...'\")\n",
    "\n",
    "    print(f\"\\n--- 正在加载模型: {MODEL_ID} ---\")\n",
    "    print(\"--- 首次运行会自动从Hugging Face Hub下载模型（约1.3GB），请耐心等待... ---\")\n",
    "    model = SentenceTransformer(MODEL_ID)\n",
    "    print(\"--- 模型加载成功。---\")\n",
    "\n",
    "    print(\"\\n--- 开始提取文档向量... ---\")\n",
    "    print(f\"--- 这将处理 {len(pseudo_raw_documents)} 篇文档，可能需要较长时间... ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    document_vectors = model.encode(\n",
    "        pseudo_raw_documents, \n",
    "        show_progress_bar=True, \n",
    "        batch_size=32  \n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 向量提取完成！耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    print(\"\\n--- 正在将向量保存到文件... ---\")\n",
    "    np.save(OUTPUT_VECTORS_PATH, document_vectors)\n",
    "    print(f\"成功将向量保存到: {OUTPUT_VECTORS_PATH}\")\n",
    "    print(f\"向量的形状为: {document_vectors.shape}\")\n",
    "    print(f\"这代表 {document_vectors.shape[0]} 篇文档，每篇由一个 {document_vectors.shape[1]} 维的向量表示。\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    extract_document_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5322a28-1e51-4385-95eb-a80868881d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRL dimensionality reduction techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a88411-9f44-4e4a-95b6-9afda5370f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_VECTORS_PATH = \"data/doc_vectors.npy\"   \n",
    "SAVE_MODEL_PATH    = \"data/4-1/CRL/model/contrastive_model.pt\"   \n",
    "\n",
    "EPOCHS     = 20\n",
    "BATCH_SIZE = 512\n",
    "LR         = 1e-3\n",
    "TEMPERATURE = 0.5\n",
    "\n",
    "INPUT_DIM  = 1024   \n",
    "HIDDEN_DIM = 512\n",
    "PROJ_DIM   = 128\n",
    "\n",
    "\n",
    "def augment(v, noise_scale=0.01, drop_prob=0.05):\n",
    "    mask = (torch.rand_like(v) > drop_prob).float()\n",
    "    v_dropped = v * mask\n",
    "\n",
    "    noise = torch.randn_like(v_dropped) * noise_scale\n",
    "    return v_dropped + noise\n",
    "\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, vectors):\n",
    "        self.vectors = torch.tensor(vectors, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.vectors[idx]\n",
    "        return augment(x), augment(x)\n",
    "\n",
    "\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, proj_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),   \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return F.normalize(z, dim=-1)\n",
    "\n",
    "def nt_xent_loss(z1, z2, temperature=0.5):\n",
    "    N = z1.size(0)\n",
    "    z = torch.cat([z1, z2], dim=0)          \n",
    "    sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=-1)\n",
    "    sim = sim / temperature\n",
    "\n",
    "    mask = torch.eye(2*N, device=z.device).bool()\n",
    "    sim.masked_fill_(mask, -9e15)\n",
    "\n",
    "    labels = torch.arange(N, device=z.device)\n",
    "    labels = torch.cat([labels + N, labels], dim=0)\n",
    "\n",
    "    return F.cross_entropy(sim, labels)\n",
    "\n",
    "\n",
    "def train_model(vectors):\n",
    "    dataset = ContrastiveDataset(vectors)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    model = ContrastiveModel(INPUT_DIM, HIDDEN_DIM, PROJ_DIM).cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=len(loader)*EPOCHS\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for x1, x2 in tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            x1, x2 = x1.cuda(), x2.cuda()\n",
    "            z1, z2 = model(x1), model(x2)\n",
    "            loss = nt_xent_loss(z1, z2, TEMPERATURE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: loss={total_loss/len(loader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), SAVE_MODEL_PATH)\n",
    "    print(f\"模型已保存到 {SAVE_MODEL_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    vectors = np.load(INPUT_VECTORS_PATH)\n",
    "    train_model(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783ce45-bf59-4327-946d-699e268e8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "INPUT_VECTORS_PATH = \"data/doc_vectors.npy\"\n",
    "SAVED_MODEL_PATH   = \"data/4-1/CRL/model/contrastive_model.pt\"\n",
    "OUTPUT_REDUCED_VECTORS_PATH = \"data/4-1/CRL/reduced_vectors_128d.npy\"\n",
    "\n",
    "INPUT_DIM  = 1024\n",
    "HIDDEN_DIM = 512\n",
    "PROJ_DIM   = 128 \n",
    "BATCH_SIZE = 1024 \n",
    "\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, proj_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return F.normalize(z, dim=-1)\n",
    "\n",
    "def run_inference():\n",
    "    if not os.path.exists(INPUT_VECTORS_PATH):\n",
    "        print(f\"错误: 输入向量文件未找到 -> {INPUT_VECTORS_PATH}\")\n",
    "        return\n",
    "    if not os.path.exists(SAVED_MODEL_PATH):\n",
    "        print(f\"错误: 训练好的模型文件未找到 -> {SAVED_MODEL_PATH}\")\n",
    "        return\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"正在使用设备: {device}\")\n",
    "\n",
    "    print(f\"正在从 {SAVED_MODEL_PATH} 加载模型...\")\n",
    "    model = ContrastiveModel(INPUT_DIM, HIDDEN_DIM, PROJ_DIM).to(device)\n",
    "    model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=device))\n",
    "\n",
    "    model.eval()\n",
    "    print(\"模型加载成功，已切换到评估模式。\")\n",
    "\n",
    "    print(f\"正在从 {INPUT_VECTORS_PATH} 加载原始向量...\")\n",
    "    original_vectors = np.load(INPUT_VECTORS_PATH)\n",
    "\n",
    "    dataset = TensorDataset(torch.tensor(original_vectors, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(f\"成功加载 {len(original_vectors)} 个向量，准备开始降维...\")\n",
    "\n",
    "    all_reduced_vectors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in tqdm(loader, desc=\"[降维中]\"):\n",
    "            batch = batch.to(device)\n",
    "            reduced_batch = model(batch)\n",
    "            all_reduced_vectors.append(reduced_batch.cpu().numpy())\n",
    "\n",
    "    print(\"降维完成，正在整理并保存结果...\")\n",
    "    final_vectors = np.concatenate(all_reduced_vectors, axis=0)\n",
    "\n",
    "    output_dir = os.path.dirname(OUTPUT_REDUCED_VECTORS_PATH)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    np.save(OUTPUT_REDUCED_VECTORS_PATH, final_vectors)\n",
    "\n",
    "    print(\"\\n操作成功！\")\n",
    "    print(f\"原始向量形状: {original_vectors.shape}\")\n",
    "    print(f\"降维后向量形状: {final_vectors.shape}\")\n",
    "    print(f\"降维后的向量已保存到: {OUTPUT_REDUCED_VECTORS_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc26c85-f631-4b3a-b898-1ef96c727062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA＆UMAP 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206df332-3f41-437b-bf01-1bb39b23aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import os\n",
    "import time\n",
    "\n",
    "INPUT_VECTORS_PATH = \"data/doc_vectors.npy\"\n",
    "\n",
    "OUTPUT_DIR = \"data/4-1/\"\n",
    "OUTPUT_PCA_PATH = os.path.join(OUTPUT_DIR, \"PCA/reduced_vectors_pca_128d.npy\")\n",
    "OUTPUT_PCA_NORM_PATH = os.path.join(OUTPUT_DIR, \"PCA/reduced_vectors_pca_128d_norm.npy\")\n",
    "OUTPUT_UMAP_PATH = os.path.join(OUTPUT_DIR, \"UMAP/reduced_vectors_umap_128d.npy\")\n",
    "OUTPUT_UMAP_NORM_PATH = os.path.join(OUTPUT_DIR, \"UMAP/reduced_vectors_umap_128d_norm.npy\")\n",
    "\n",
    "TARGET_DIM = 128\n",
    "UMAP_N_NEIGHBORS = 15\n",
    "UMAP_MIN_DIST = 0.1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def normalize_l2_numpy(vectors: np.ndarray) -> np.ndarray:  \n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)   \n",
    "    norms = np.where(norms == 0, 1e-8, norms)\n",
    "    normalized_vectors = vectors / norms    \n",
    "    return normalized_vectors\n",
    "\n",
    "def reduce_with_pca(vectors, target_dim):\n",
    "    print(\"\\n--- 开始执行 PCA 降维 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pca = PCA(n_components=target_dim, random_state=RANDOM_STATE)\n",
    "    reduced_vectors = pca.fit_transform(vectors)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"PCA 保留的总方差比例: {explained_variance:.4f}\")\n",
    "    print(f\"PCA 降维完成，耗时: {end_time - start_time:.2f} 秒\")\n",
    "    \n",
    "    return reduced_vectors\n",
    "\n",
    "\n",
    "def reduce_with_umap(vectors, target_dim):\n",
    "    print(\"\\n--- 开始执行 UMAP 降维 (使用 cosine 度量) ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_components=target_dim,\n",
    "        n_neighbors=UMAP_N_NEIGHBORS,\n",
    "        min_dist=UMAP_MIN_DIST,\n",
    "        metric='cosine',\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    reduced_vectors = reducer.fit_transform(vectors)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"UMAP 降维完成，耗时: {end_time - start_time:.2f} 秒\")\n",
    "    \n",
    "    return reduced_vectors\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(INPUT_VECTORS_PATH):\n",
    "        print(f\"错误: 输入文件未找到 -> {INPUT_VECTORS_PATH}\")\n",
    "    else:\n",
    "        print(f\"正在从 {INPUT_VECTORS_PATH} 加载原始向量...\")\n",
    "        original_vectors = np.load(INPUT_VECTORS_PATH).astype(np.float32)\n",
    "        print(f\"成功加载 {original_vectors.shape[0]} 个向量，原始维度为 {original_vectors.shape[1]}\")\n",
    "\n",
    "        pca_reduced_vectors = reduce_with_pca(original_vectors, TARGET_DIM)\n",
    "\n",
    "        np.save(OUTPUT_PCA_PATH, pca_reduced_vectors)\n",
    "        print(f\"PCA (未归一化) 向量已保存到: {OUTPUT_PCA_PATH}\")\n",
    "        print(f\"   -> 形状: {pca_reduced_vectors.shape}\")\n",
    "\n",
    "        print(\"   -> 正在对PCA输出进行L2归一化...\")\n",
    "        pca_reduced_vectors_norm = normalize_l2_numpy(pca_reduced_vectors)\n",
    "        \n",
    "        np.save(OUTPUT_PCA_NORM_PATH, pca_reduced_vectors_norm)\n",
    "        print(f\"PCA (L2归一化) 向量已保存到: {OUTPUT_PCA_NORM_PATH}\")\n",
    "        print(f\"   -> 形状: {pca_reduced_vectors_norm.shape}\")\n",
    "\n",
    "        umap_reduced_vectors = reduce_with_umap(original_vectors, TARGET_DIM)\n",
    "\n",
    "        np.save(OUTPUT_UMAP_PATH, umap_reduced_vectors)\n",
    "        print(f\"\\nUMAP (未归一化) 向量已保存到: {OUTPUT_UMAP_PATH}\")\n",
    "        print(f\"   -> 形状: {umap_reduced_vectors.shape}\")\n",
    "\n",
    "        print(\"   -> 正在对UMAP输出进行L2归一化...\")\n",
    "        umap_reduced_vectors_norm = normalize_l2_numpy(umap_reduced_vectors)\n",
    "        \n",
    "        np.save(OUTPUT_UMAP_NORM_PATH, umap_reduced_vectors_norm)\n",
    "        print(f\"UMAP (L2归一化) 向量已保存到: {OUTPUT_UMAP_NORM_PATH}\")\n",
    "        print(f\"   -> 形状: {umap_reduced_vectors_norm.shape}\")\n",
    "\n",
    "        print(\"\\n所有降维任务已完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541060e6-e2b7-4c6f-9eeb-49c23a5f2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731254dc-95b7-4819-b579-2f78d86de3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import time\n",
    "\n",
    "INPUT_VECTORS_PATH = \"data/doc_vectors.npy\"\n",
    "OUTPUT_DIR = \"data/4-1/64/\"\n",
    "\n",
    "TARGET_DIMS_LIST = [64]\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def normalize_l2_numpy(vectors: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    norms = np.where(norms == 0, 1e-8, norms)\n",
    "    normalized_vectors = vectors / norms\n",
    "    return normalized_vectors\n",
    "\n",
    "def reduce_with_pca(vectors, target_dim):\n",
    "    print(f\"\\n--- 开始执行 PCA 降维至 {target_dim} 维 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pca = PCA(n_components=target_dim, random_state=RANDOM_STATE)\n",
    "    reduced_vectors = pca.fit_transform(vectors)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    \n",
    "    print(f\"目标维度: {target_dim}\")\n",
    "    print(f\"保留的总方差比例: {explained_variance:.4f}\")\n",
    "    print(f\"PCA 降维完成，耗时: {end_time - start_time:.2f} 秒\")\n",
    "    \n",
    "    return reduced_vectors\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(INPUT_VECTORS_PATH):\n",
    "        print(f\"错误: 输入文件未找到 -> {INPUT_VECTORS_PATH}\")\n",
    "    else:\n",
    "        print(f\"正在从 {INPUT_VECTORS_PATH} 加载原始向量...\")\n",
    "        original_vectors = np.load(INPUT_VECTORS_PATH).astype(np.float32)\n",
    "        print(f\"成功加载 {original_vectors.shape[0]} 个向量，原始维度为 {original_vectors.shape[1]}\")\n",
    "\n",
    "        for dim in TARGET_DIMS_LIST:\n",
    "            pca_reduced_vectors = reduce_with_pca(original_vectors, dim)\n",
    "            \n",
    "            print(\"   -> 正在对PCA输出进行L2归一化...\")\n",
    "            pca_reduced_vectors_norm = normalize_l2_numpy(pca_reduced_vectors)\n",
    "            \n",
    "            output_filename = f\"reduced_vectors_pca_{dim}d_norm.npy\"\n",
    "            output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "            \n",
    "            np.save(output_path, pca_reduced_vectors_norm)\n",
    "            \n",
    "            print(f\"PCA (L2归一化) 向量已保存到: {output_path}\")\n",
    "            print(f\"   -> 形状: {pca_reduced_vectors_norm.shape}\")\n",
    "\n",
    "        print(\"\\n所有维度的降维任务已全部完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f8982-5ce5-452b-aac4-5116d139df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c5b29-87e2-4ed9-afee-0959bbe1b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import time\n",
    "\n",
    "INPUT_VECTORS_PATH = \"data/doc_vectors.npy\"\n",
    "\n",
    "OUTPUT_DIR = \"data/4-1/256/\"\n",
    "\n",
    "TARGET_DIMS_LIST = [256]\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def normalize_l2_numpy(vectors: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    norms = np.where(norms == 0, 1e-8, norms)\n",
    "    normalized_vectors = vectors / norms\n",
    "    return normalized_vectors\n",
    "\n",
    "def reduce_with_pca(vectors, target_dim):\n",
    "    print(f\"\\n--- 开始执行 PCA 降维至 {target_dim} 维 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pca = PCA(n_components=target_dim, random_state=RANDOM_STATE)\n",
    "    reduced_vectors = pca.fit_transform(vectors)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    \n",
    "    print(f\"目标维度: {target_dim}\")\n",
    "    print(f\"保留的总方差比例: {explained_variance:.4f}\")\n",
    "    print(f\"PCA 降维完成，耗时: {end_time - start_time:.2f} 秒\")\n",
    "    \n",
    "    return reduced_vectors\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(INPUT_VECTORS_PATH):\n",
    "        print(f\"错误: 输入文件未找到 -> {INPUT_VECTORS_PATH}\")\n",
    "    else:\n",
    "        print(f\"正在从 {INPUT_VECTORS_PATH} 加载原始向量...\")\n",
    "        original_vectors = np.load(INPUT_VECTORS_PATH).astype(np.float32)\n",
    "        print(f\"成功加载 {original_vectors.shape[0]} 个向量，原始维度为 {original_vectors.shape[1]}\")\n",
    "\n",
    "        for dim in TARGET_DIMS_LIST:\n",
    "            pca_reduced_vectors = reduce_with_pca(original_vectors, dim)\n",
    "\n",
    "            print(\"   -> 正在对PCA输出进行L2归一化...\")\n",
    "            pca_reduced_vectors_norm = normalize_l2_numpy(pca_reduced_vectors)\n",
    "\n",
    "            output_filename = f\"reduced_vectors_pca_{dim}d_norm.npy\"\n",
    "            output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "            \n",
    "            np.save(output_path, pca_reduced_vectors_norm)\n",
    "            \n",
    "            print(f\"PCA (L2归一化) 向量已保存到: {output_path}\")\n",
    "            print(f\"   -> 形状: {pca_reduced_vectors_norm.shape}\")\n",
    "\n",
    "        print(\"\\n所有维度的降维任务已全部完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
