{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dff17d-15d0-44be-ad3c-a5aa9aa630ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/paragraph.pkl' \n",
    "FILTERED_DICTIONARY_PATH = 'data/LDA/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/LDA/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10 \n",
    "NO_ABOVE = 0.5\n",
    "KEEP_N = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify() \n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ae9fd-2105-480c-b062-75b768b5cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/LDA/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/LDA/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/LDA/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,               \n",
    "            num_topics=n_topics,          \n",
    "            id2word=dictionary,            \n",
    "            random_state=42,              \n",
    "            passes=10,                   \n",
    "            workers=num_workers,         \n",
    "        )\n",
    "        \n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58ba07-13f6-4fad-8c55-e9a8c03df6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/paragraph.pkl' \n",
    "FINAL_DICTIONARY_PATH = 'data/LDA/final_dictionary.dict'\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/LDA/model/'\n",
    "RESULTS_CSV_PATH = 'data/LDA/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主执行流程\"\"\"\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "    \n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
