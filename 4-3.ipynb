{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec89107-f21c-4092-a965-148fa435d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FADC-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3fb23-ed63-4f0d-9ff4-b3f8b8c07d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/base/pca_128_hd_agc_results.pkl'\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "OUTPUT_DIR = 'data/4-3/base/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f96cc-1cc7-4d49-acb8-bcd80c00101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/base/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/base/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/base/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "    \n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd873569-0ea3-4080-a0b8-c6e3f7c3bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/base/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/base/final_corpus.pkl'\n",
    "MODEL_SAVE_DIR = 'data/4-3/base/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "        \n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,             \n",
    "            num_topics=n_topics,      \n",
    "            id2word=dictionary,          \n",
    "            random_state=42,             \n",
    "            passes=10,                 \n",
    "            workers=num_workers,          \n",
    "        )\n",
    "        \n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a7551-51e9-47cf-8472-9a6b421a220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/base/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/base/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/base/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/base/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c45ad-0d8a-4176-9e65-9313b55cdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8d73b-f309-4408-afd8-5f84f1ee1047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/4_80/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/4_80/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82bb94-9fc4-4329-9078-69c1da9dee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/4_83/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/4_83/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ac8bc-5303-4837-91d5-40aa2949b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/4_86/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/4_86/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06000d-0fc4-42d0-9595-5bcb1b56cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/6_80/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/6_80/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e532821-024e-4f03-92d1-7e22b8d2583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/6_83/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/6_83/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba039af-12b6-4261-a5b0-fc4b7e14aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/6_86/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/6_86/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3badfc51-7e32-4864-a7b8-a76fa674c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/8_80/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/8_80/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b332fab-b587-41ec-b92f-cc20e1692625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/8_83/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/8_83/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8a876-449c-474f-b3a8-b3c91673f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/8_86/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/8_86/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f6495-b11a-494c-ad40-dc5cb172e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/10_80/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/10_80/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152154d7-4a42-4d93-9744-d94793ccff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/10_83/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/10_83/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea1702-cd4a-49fe-ad40-5257a8a2d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-3/10_86/pca_128_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-3/10_86/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3f0f9-da31-42ab-a154-9f77726cea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/4_80/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/4_80/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/4_80/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942ca8a-bd9b-42a2-ab1b-843fd21ed204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/4_83/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/4_83/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/4_83/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751f5ed-6fd0-4cdf-91d1-b4de52d0ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/4_86/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/4_86/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/4_86/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a657a2-4ae1-42ba-b1b7-52fac03d7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/6_80/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/6_80/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/6_80/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd98f43-e18a-427b-8116-e896f554a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/6_83/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/6_83/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/6_83/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140bbf9-284b-4cbc-97ee-bbb126301bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/6_86/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/6_86/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/6_86/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac81e90-0fc4-46d8-8077-0ba3a34e5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/8_80/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/8_80/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/8_80/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89145916-4bff-4721-b56e-799a325400da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/8_83/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/8_83/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/8_83/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da72a9e-95a5-43cf-991a-1405fef54bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/8_86/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/8_86/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/8_86/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec7621-8512-495e-949d-f4615caf36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/10_80/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/10_80/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/10_80/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328f455-fab0-4f27-8f7b-8cdc21a46381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/10_83/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/10_83/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/10_83/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f26026-0255-40a5-b160-fb008cda3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/10_86/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-3/10_86/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-3/10_86/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e5edc-2c6b-48ce-b0d5-a65aa174be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/4_80/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/4_80/final_corpus.pkl'\n",
    "\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/4_80/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "        \n",
    "        \n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,                \n",
    "            num_topics=n_topics,          \n",
    "            id2word=dictionary,           \n",
    "            random_state=42,               \n",
    "            passes=10,                     \n",
    "            workers=num_workers,           \n",
    "            \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148e8bb-040c-485c-8d9c-850267897746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/4_83/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/4_83/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/4_83/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "        \n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,                \n",
    "            num_topics=n_topics,         \n",
    "            id2word=dictionary,            \n",
    "            random_state=42,               \n",
    "            passes=10,                     \n",
    "            workers=num_workers,          \n",
    "\n",
    "        )\n",
    "        \n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0b4a7-09f0-4994-909a-f7061f46ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/4_86/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/4_86/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/4_86/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,                \n",
    "            num_topics=n_topics,           \n",
    "            id2word=dictionary,           \n",
    "            random_state=42,              \n",
    "            passes=10,                     \n",
    "            workers=num_workers,           \n",
    "\n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3e62a-d35d-4c75-a486-85807c7bb090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/6_80/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/6_80/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/6_80/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,                \n",
    "            num_topics=n_topics,           \n",
    "            id2word=dictionary,            \n",
    "            random_state=42,              \n",
    "            passes=10,                    \n",
    "            workers=num_workers,          \n",
    "\n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca641005-3db0-4ad2-80bd-85cfbe7e529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/6_83/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/6_83/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/6_83/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,            \n",
    "            num_topics=n_topics,      \n",
    "            id2word=dictionary,        \n",
    "            random_state=42,             \n",
    "            passes=10,                  \n",
    "            workers=num_workers,       \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6842c3-e6b0-45c5-a033-61cac96b9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/6_86/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/6_86/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/6_86/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,          \n",
    "            num_topics=n_topics,        \n",
    "            id2word=dictionary,          \n",
    "            random_state=42,            \n",
    "            passes=10,                  \n",
    "            workers=num_workers,          \n",
    "\n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e00cf-b056-41d2-89c6-2549a051bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/8_80/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/8_80/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/8_80/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,             \n",
    "            num_topics=n_topics,          \n",
    "            id2word=dictionary,        \n",
    "            random_state=42,           \n",
    "            passes=10,               \n",
    "            workers=num_workers,           \n",
    "            \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af107a42-0501-4e19-b417-e85391b0fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/8_83/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/8_83/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/8_83/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,           \n",
    "            num_topics=n_topics,      \n",
    "            id2word=dictionary,          \n",
    "            random_state=42,            \n",
    "            passes=10,                  \n",
    "            workers=num_workers,      \n",
    "            \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c3133-3ce5-47d5-85ab-9ad0abe2ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/8_86/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/8_86/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/8_86/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,             \n",
    "            num_topics=n_topics,         \n",
    "            id2word=dictionary,         \n",
    "            random_state=42,             \n",
    "            passes=10,                 \n",
    "            workers=num_workers,        \n",
    "\n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea23028-c4e7-4ecc-8e6f-012293dcb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/10_80/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/10_80/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/10_80/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,             \n",
    "            num_topics=n_topics,       \n",
    "            id2word=dictionary,         \n",
    "            random_state=42,             \n",
    "            passes=10,                 \n",
    "            workers=num_workers,      \n",
    "            \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9bfdac-5d55-4ba9-b923-21217cfb6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/10_83/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/10_83/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/10_83/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,            \n",
    "            num_topics=n_topics,        \n",
    "            id2word=dictionary,           \n",
    "            random_state=42,             \n",
    "            passes=10,                   \n",
    "            workers=num_workers,         \n",
    "\n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3cf30-5cf5-4f56-93fe-6284526d5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/10_86/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-3/10_86/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/10_86/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,                \n",
    "            num_topics=n_topics,          \n",
    "            id2word=dictionary,         \n",
    "            random_state=42,            \n",
    "            passes=10,                    \n",
    "            workers=num_workers,       \n",
    "\n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728b852-2eb9-41a0-aa5e-7c0790081979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/4_80/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/4_80/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/4_80/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/4_80/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353fed3-41f3-48d0-aaec-5ee060a60395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/4_83/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/4_83/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/4_83/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/4_83/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ed108-286f-4ad6-a4f5-08b174089b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/4_86/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/4_86/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/4_86/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/4_86/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3894dcb5-b0df-472c-8bfd-f652d1a3b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/6_80/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/6_80/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/6_80/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/6_80/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236fdcf-64ee-4284-94e0-6c37314e6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/6_83/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/6_83/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/6_83/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/6_83/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e44899-3080-43b0-883e-8d196a061edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/6_86/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/6_86/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/6_86/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/6_86/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "    \n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1eba21-170e-4c85-82f5-8c2abcdaef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/8_80/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/8_80/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/8_80/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/8_80/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "    \n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e1097-59f1-4e8c-9837-6c4156664617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/8_83/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/8_83/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/8_83/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/8_83/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "    \n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "    \n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929b99d-ae5d-4627-94db-e16c6c53e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/8_86/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/8_86/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/8_86/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/8_86/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85665a0-0896-4d90-8ee1-b017229a1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/10_80/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/10_80/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/10_80/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/10_80/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d024d-30b1-4ec3-b7a1-e43f4244be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/10_83/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/10_83/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/10_83/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/10_83/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146433d-f456-4c21-ae44-8ecfb181b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-3/10_86/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-3/10_86/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-3/10_86/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-3/10_86/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "        \n",
    "        # 确保结果目录存在\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octis",
   "language": "python",
   "name": "octis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
