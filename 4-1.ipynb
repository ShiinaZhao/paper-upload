{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff8430-9092-42b0-937c-f28ae47ebd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduction Technique  NO-DR CRL UMAP PCA at dimension 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b6809-5a94-4635-9671-4caeaac017d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO-DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83950e8-3757-4711-999a-a95566ace7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-1/O/hd_agc_results.pkl'\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "OUTPUT_DIR = 'data/4-1/O/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\" 从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\" 详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\" 成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf7de9-55fd-4ffd-aaff-b268bf5b1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/O/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-1/O/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-1/O/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36762bab-5d52-41fd-baa3-c926cefb8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/O/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-1/O/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-1/O/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,              \n",
    "            num_topics=n_topics,           \n",
    "            id2word=dictionary,           \n",
    "            random_state=42,             \n",
    "            passes=10,                   \n",
    "            workers=num_workers,          \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9c3fa-4c52-4a75-9382-36d45ae27d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/O/merged_documents_with_clusters.pkl' \n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/O/final_dictionary.dict'\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "MODEL_SAVE_DIR = 'data/4-1/O/model/'\n",
    "RESULTS_CSV_PATH = 'data/4-1/O/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45216638-d0eb-458d-9aea-74177e1f080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece2bfc-ed8a-4e3d-9c80-b8cbc66a02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-1/CRL/hd_agc_results.pkl'\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "OUTPUT_DIR = 'data/4-1/CRL/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\" 从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\" 详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\" 成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a8ec0-4d83-4b8f-b946-062e31d4c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/CRL/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-1/CRL/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-1/CRL/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e120d0-2fc7-47dd-b043-e77ce727b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/CRL/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-1/CRL/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-1/CRL/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,              \n",
    "            num_topics=n_topics,           \n",
    "            id2word=dictionary,           \n",
    "            random_state=42,             \n",
    "            passes=10,                   \n",
    "            workers=num_workers,          \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57daa3a-b974-4db6-8e76-6c2154c6ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/CRL/merged_documents_with_clusters.pkl' \n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/CRL/final_dictionary.dict'\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "MODEL_SAVE_DIR = 'data/4-1/CRL/model/'\n",
    "RESULTS_CSV_PATH = 'data/4-1/CRL/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b21829-dfb9-4f30-8999-dfab33e7683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d417da0-02fa-4e67-ab30-19bf311befe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-1/UMAP/hd_agc_results.pkl'\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "OUTPUT_DIR = 'data/4-1/UMAP/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\" 从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\" 详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\" 成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be5ec6-21ec-4796-be16-2ab144bda65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os  \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/UMAP/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-1/UMAP/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-1/UMAP/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed5f1a-52de-4f2d-9a47-57e89c532a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/UMAP/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-1/UMAP/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-1/UMAP/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,              \n",
    "            num_topics=n_topics,           \n",
    "            id2word=dictionary,           \n",
    "            random_state=42,             \n",
    "            passes=10,                   \n",
    "            workers=num_workers,          \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427dae7a-1afa-46b5-9ee2-3e5c7608afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/UMAP/merged_documents_with_clusters.pkl' \n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/UMAP/final_dictionary.dict'\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "MODEL_SAVE_DIR = 'data/4-1/UMAP/model/'\n",
    "RESULTS_CSV_PATH = 'data/4-1/UMAP/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb4d56-0814-4a8b-bd09-2d5bbfb19a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5f776-703b-4b0c-b498-2fbac9a3aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-1/PCA/hd_agc_results.pkl'\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "OUTPUT_DIR = 'data/4-1/PCA/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\" 从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\" 详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\" 成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa43f9-a956-4953-a935-bfd8dc96d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-1/PCA/hd_agc_results.pkl'\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "OUTPUT_DIR = 'data/4-1/PCA/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\" 从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\" 详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\" 成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481c773-3354-49fe-97b7-642e087eb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/PCA/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-1/PCA/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-1/PCA/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,              \n",
    "            num_topics=n_topics,           \n",
    "            id2word=dictionary,           \n",
    "            random_state=42,             \n",
    "            passes=10,                   \n",
    "            workers=num_workers,          \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ae6d0-2372-47fa-9660-dfc24573f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/PCA/merged_documents_with_clusters.pkl' \n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/PCA/final_dictionary.dict'\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "MODEL_SAVE_DIR = 'data/4-1/PCA/model/'\n",
    "RESULTS_CSV_PATH = 'data/4-1/PCA/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c31d59-6e14-4976-a2a5-8f4d8aa789bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164d6b3-752c-40f8-b84d-127167f1ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator, FuncFormatter\n",
    "\n",
    "INPUT_CLUSTER_RESULTS_PATH = 'data/4-1/O/hd_agc_results.pkl'\n",
    "INPUT_VECTORS_PATH = 'data/doc_vectors.npy'\n",
    "\n",
    "OUTPUT_FIGURE_PATH = 'data/4-1/O/similarity_distribution.png' \n",
    "\n",
    "S_MIN_THRESHOLD = 0.83\n",
    "\n",
    "AXIS_LABEL_FONTSIZE = 18\n",
    "LEGEND_FONTSIZE = 18\n",
    "TICK_LABEL_FONTSIZE = 18\n",
    "SUB_LABEL_FONTSIZE = 28\n",
    "\n",
    "\n",
    "def evaluate_cluster_similarity():\n",
    "    print(\"--- 正在加载数据... ---\")   \n",
    "    if not os.path.exists(INPUT_CLUSTER_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"聚类结果文件未找到: {INPUT_CLUSTER_RESULTS_PATH}\")\n",
    "    if not os.path.exists(INPUT_VECTORS_PATH):\n",
    "        raise FileNotFoundError(f\"向量文件未找到: {INPUT_VECTORS_PATH}\")\n",
    "\n",
    "    with open(INPUT_CLUSTER_RESULTS_PATH, 'rb') as f:\n",
    "        cluster_results = pickle.load(f)\n",
    "    clusters = cluster_results['clusters']\n",
    "    print(f\"成功加载 {len(clusters)} 个簇。\")\n",
    "\n",
    "    vectors = np.load(INPUT_VECTORS_PATH)\n",
    "    if vectors.dtype != np.float32:\n",
    "        vectors = vectors.astype(np.float32)\n",
    "    print(f\"成功加载 {len(vectors)} 个向量。\")\n",
    "\n",
    "    print(\"\\n--- 正在计算簇内相似度... ---\")\n",
    "    all_intra_cluster_similarities = []\n",
    "\n",
    "    for cluster_indices in tqdm(clusters, desc=\"[计算中]\"):\n",
    "        if len(cluster_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        cluster_vectors = vectors[cluster_indices]\n",
    "        sim_matrix = cosine_similarity(cluster_vectors)\n",
    "        upper_triangle_indices = np.triu_indices(len(cluster_indices), k=1)\n",
    "        pairwise_sims = sim_matrix[upper_triangle_indices]\n",
    "        all_intra_cluster_similarities.extend(pairwise_sims.tolist())\n",
    "\n",
    "    if not all_intra_cluster_similarities:\n",
    "        print(\"\\n错误：未能计算出任何相似度分数。请检查簇是否都过小（成员数<2）。\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- 计算完成！共得到 {len(all_intra_cluster_similarities):,} 个相似度分数。---\")\n",
    "\n",
    "    print(\"--- 正在生成相似度分布图... ---\")\n",
    "    \n",
    "    sim_array = np.array(all_intra_cluster_similarities)\n",
    "    \n",
    "    print(f\"  - 平均相似度: {np.mean(sim_array):.4f}\")\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    sns.histplot(sim_array, bins=100, kde=True, color='skyblue', alpha=0.7, ax=ax)\n",
    "    \n",
    "    if S_MIN_THRESHOLD is not None:\n",
    "        plt.axvline(x=S_MIN_THRESHOLD, color='r', linestyle='--', linewidth=2, \n",
    "                    label=f'S_MIN Threshold = {S_MIN_THRESHOLD}')\n",
    "        plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "        \n",
    "\n",
    "    plt.xlabel('Cosine Similarity', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "    \n",
    "    ax.text(0.5, -0.2, '(a)', \n",
    "            transform=ax.transAxes, \n",
    "            ha='center', \n",
    "            va='top', \n",
    "            fontsize=SUB_LABEL_FONTSIZE)\n",
    "    \n",
    "    plt.ylabel('Frequency', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "    \n",
    "    plt.tick_params(axis='both', which='major', labelsize=TICK_LABEL_FONTSIZE)\n",
    "    \n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.ylim(0, 20000)\n",
    "\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "\n",
    "    def custom_tick_formatter(x, pos):\n",
    "        if x == 0:\n",
    "            return '0'\n",
    "        else:\n",
    "            return f'{x:.1f}'\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(custom_tick_formatter))\n",
    "\n",
    "    output_dir = os.path.dirname(OUTPUT_FIGURE_PATH)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"已创建输出目录: {output_dir}\")\n",
    "\n",
    "    plt.tight_layout(pad=1.5)\n",
    "    \n",
    "    plt.savefig(OUTPUT_FIGURE_PATH, dpi=300)\n",
    "    \n",
    "    print(f\"\\n评估完成，图像已成功保存至: {OUTPUT_FIGURE_PATH}\")\n",
    "    \n",
    "    print(\"正在显示相似度分布图...\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_cluster_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0319b915-b9e8-41e6-a272-36cb3c71c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator, FuncFormatter\n",
    "\n",
    "\n",
    "INPUT_CLUSTER_RESULTS_PATH = 'data/4-1/CRL/hd_agc_results.pkl'\n",
    "INPUT_VECTORS_PATH = 'data/doc_vectors.npy'\n",
    "\n",
    "OUTPUT_FIGURE_PATH = 'data/4-1/CRL/similarity_distribution.png' \n",
    "\n",
    "S_MIN_THRESHOLD = 0.83\n",
    "\n",
    "AXIS_LABEL_FONTSIZE = 18\n",
    "LEGEND_FONTSIZE = 18\n",
    "TICK_LABEL_FONTSIZE = 18\n",
    "SUB_LABEL_FONTSIZE = 28\n",
    "\n",
    "def evaluate_cluster_similarity():\n",
    "    print(\"--- 正在加载数据... ---\")\n",
    "    \n",
    "    if not os.path.exists(INPUT_CLUSTER_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"聚类结果文件未找到: {INPUT_CLUSTER_RESULTS_PATH}\")\n",
    "    if not os.path.exists(INPUT_VECTORS_PATH):\n",
    "        raise FileNotFoundError(f\"向量文件未找到: {INPUT_VECTORS_PATH}\")\n",
    "\n",
    "    with open(INPUT_CLUSTER_RESULTS_PATH, 'rb') as f:\n",
    "        cluster_results = pickle.load(f)\n",
    "    clusters = cluster_results['clusters']\n",
    "    print(f\"成功加载 {len(clusters)} 个簇。\")\n",
    "\n",
    "    vectors = np.load(INPUT_VECTORS_PATH)\n",
    "    if vectors.dtype != np.float32:\n",
    "        vectors = vectors.astype(np.float32)\n",
    "    print(f\"成功加载 {len(vectors)} 个向量。\")\n",
    "\n",
    "    print(\"\\n--- 正在计算簇内相似度... ---\")\n",
    "    all_intra_cluster_similarities = []\n",
    "\n",
    "    for cluster_indices in tqdm(clusters, desc=\"[计算中]\"):\n",
    "        if len(cluster_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        cluster_vectors = vectors[cluster_indices]\n",
    "        sim_matrix = cosine_similarity(cluster_vectors)\n",
    "        upper_triangle_indices = np.triu_indices(len(cluster_indices), k=1)\n",
    "        pairwise_sims = sim_matrix[upper_triangle_indices]\n",
    "\n",
    "        all_intra_cluster_similarities.extend(pairwise_sims.tolist())\n",
    "\n",
    "    if not all_intra_cluster_similarities:\n",
    "        print(\"\\n错误：未能计算出任何相似度分数。请检查簇是否都过小（成员数<2）。\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- 计算完成！共得到 {len(all_intra_cluster_similarities):,} 个相似度分数。---\")\n",
    "\n",
    "    print(\"--- 正在生成相似度分布图... ---\")\n",
    "    \n",
    "    sim_array = np.array(all_intra_cluster_similarities)\n",
    "    \n",
    "    print(f\"  - 平均相似度: {np.mean(sim_array):.4f}\")\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    sns.histplot(sim_array, bins=100, kde=True, color='skyblue', alpha=0.7, ax=ax)\n",
    "    \n",
    "    if S_MIN_THRESHOLD is not None:\n",
    "        plt.axvline(x=S_MIN_THRESHOLD, color='r', linestyle='--', linewidth=2, \n",
    "                    label=f'S_MIN Threshold = {S_MIN_THRESHOLD}')\n",
    "        plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "\n",
    "    plt.xlabel('Cosine Similarity', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "\n",
    "    ax.text(0.5, -0.2, '(b)', \n",
    "            transform=ax.transAxes, \n",
    "            ha='center', \n",
    "            va='top', \n",
    "            fontsize=SUB_LABEL_FONTSIZE)\n",
    "    \n",
    "    plt.ylabel('Frequency', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "    \n",
    "    plt.tick_params(axis='both', which='major', labelsize=TICK_LABEL_FONTSIZE)\n",
    "\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.ylim(0, 20000)\n",
    "\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "\n",
    "    def custom_tick_formatter(x, pos):\n",
    "        if x == 0:\n",
    "            return '0'\n",
    "        else:\n",
    "            return f'{x:.1f}'\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(custom_tick_formatter))\n",
    "\n",
    "    output_dir = os.path.dirname(OUTPUT_FIGURE_PATH)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"已创建输出目录: {output_dir}\")\n",
    "\n",
    "    plt.tight_layout(pad=1.5)\n",
    "    \n",
    "    plt.savefig(OUTPUT_FIGURE_PATH, dpi=300)\n",
    "    \n",
    "    print(f\"\\n评估完成，图像已成功保存至: {OUTPUT_FIGURE_PATH}\")\n",
    "    \n",
    "    print(\"正在显示相似度分布图...\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_cluster_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb5448-490d-40fb-8674-97259af40611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator, FuncFormatter\n",
    "\n",
    "INPUT_CLUSTER_RESULTS_PATH = 'data/4-1/UMAP/hd_agc_results.pkl'\n",
    "INPUT_VECTORS_PATH = 'data/doc_vectors.npy'\n",
    "\n",
    "OUTPUT_FIGURE_PATH = 'data/4-1/UMAP/similarity_distribution.png' \n",
    "\n",
    "S_MIN_THRESHOLD = 0.83\n",
    "AXIS_LABEL_FONTSIZE = 18\n",
    "LEGEND_FONTSIZE = 18\n",
    "TICK_LABEL_FONTSIZE = 18\n",
    "SUB_LABEL_FONTSIZE = 28\n",
    "\n",
    "def evaluate_cluster_similarity():\n",
    "    print(\"--- 正在加载数据... ---\")\n",
    "    \n",
    "    if not os.path.exists(INPUT_CLUSTER_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"聚类结果文件未找到: {INPUT_CLUSTER_RESULTS_PATH}\")\n",
    "    if not os.path.exists(INPUT_VECTORS_PATH):\n",
    "        raise FileNotFoundError(f\"向量文件未找到: {INPUT_VECTORS_PATH}\")\n",
    "\n",
    "    with open(INPUT_CLUSTER_RESULTS_PATH, 'rb') as f:\n",
    "        cluster_results = pickle.load(f)\n",
    "    clusters = cluster_results['clusters']\n",
    "    print(f\"成功加载 {len(clusters)} 个簇。\")\n",
    "\n",
    "    vectors = np.load(INPUT_VECTORS_PATH)\n",
    "    if vectors.dtype != np.float32:\n",
    "        vectors = vectors.astype(np.float32)\n",
    "    print(f\"成功加载 {len(vectors)} 个向量。\")\n",
    "\n",
    "    print(\"\\n--- 正在计算簇内相似度... ---\")\n",
    "    all_intra_cluster_similarities = []\n",
    "\n",
    "    for cluster_indices in tqdm(clusters, desc=\"[计算中]\"):\n",
    "        if len(cluster_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        cluster_vectors = vectors[cluster_indices]\n",
    "        sim_matrix = cosine_similarity(cluster_vectors)\n",
    "        upper_triangle_indices = np.triu_indices(len(cluster_indices), k=1)\n",
    "        pairwise_sims = sim_matrix[upper_triangle_indices]\n",
    "        all_intra_cluster_similarities.extend(pairwise_sims.tolist())\n",
    "\n",
    "    if not all_intra_cluster_similarities:\n",
    "        print(\"\\n错误：未能计算出任何相似度分数。请检查簇是否都过小（成员数<2）。\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- 计算完成！共得到 {len(all_intra_cluster_similarities):,} 个相似度分数。---\")\n",
    "\n",
    "    print(\"--- 正在生成相似度分布图... ---\")\n",
    "    \n",
    "    sim_array = np.array(all_intra_cluster_similarities)\n",
    "    \n",
    "    print(f\"  - 平均相似度: {np.mean(sim_array):.4f}\")\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    sns.histplot(sim_array, bins=100, kde=True, color='skyblue', alpha=0.7, ax=ax)\n",
    "    \n",
    "    if S_MIN_THRESHOLD is not None:\n",
    "        plt.axvline(x=S_MIN_THRESHOLD, color='r', linestyle='--', linewidth=2, \n",
    "                    label=f'S_MIN Threshold = {S_MIN_THRESHOLD}')\n",
    "        plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "\n",
    "    plt.xlabel('Cosine Similarity', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "\n",
    "    ax.text(0.5, -0.2, '(c)', \n",
    "            transform=ax.transAxes, \n",
    "            ha='center', \n",
    "            va='top', \n",
    "            fontsize=SUB_LABEL_FONTSIZE)\n",
    "    \n",
    "    plt.ylabel('Frequency', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "    \n",
    "    plt.tick_params(axis='both', which='major', labelsize=TICK_LABEL_FONTSIZE)\n",
    "\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.ylim(0, 20000)\n",
    "\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "\n",
    "    def custom_tick_formatter(x, pos):\n",
    "        if x == 0:\n",
    "            return '0'\n",
    "        else:\n",
    "            return f'{x:.1f}'\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(custom_tick_formatter))\n",
    "\n",
    "    output_dir = os.path.dirname(OUTPUT_FIGURE_PATH)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"已创建输出目录: {output_dir}\")\n",
    "\n",
    "    plt.tight_layout(pad=1.5)\n",
    "    \n",
    "    plt.savefig(OUTPUT_FIGURE_PATH, dpi=300)\n",
    "    \n",
    "    print(f\"\\n评估完成，图像已成功保存至: {OUTPUT_FIGURE_PATH}\")\n",
    "    \n",
    "    print(\"正在显示相似度分布图...\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_cluster_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24e546-ac3a-4597-a884-1e48bd474437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator, FuncFormatter\n",
    "\n",
    "INPUT_CLUSTER_RESULTS_PATH = 'data/4-1/PCA/hd_agc_results.pkl'\n",
    "INPUT_VECTORS_PATH = 'data/doc_vectors.npy'\n",
    "\n",
    "OUTPUT_FIGURE_PATH = 'data/4-1/PCA/similarity_distribution.png' \n",
    "\n",
    "S_MIN_THRESHOLD = 0.83\n",
    "\n",
    "AXIS_LABEL_FONTSIZE = 18\n",
    "LEGEND_FONTSIZE = 18\n",
    "TICK_LABEL_FONTSIZE = 18\n",
    "SUB_LABEL_FONTSIZE = 28\n",
    "\n",
    "def evaluate_cluster_similarity():\n",
    "    print(\"--- 正在加载数据... ---\")\n",
    "    \n",
    "    if not os.path.exists(INPUT_CLUSTER_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"聚类结果文件未找到: {INPUT_CLUSTER_RESULTS_PATH}\")\n",
    "    if not os.path.exists(INPUT_VECTORS_PATH):\n",
    "        raise FileNotFoundError(f\"向量文件未找到: {INPUT_VECTORS_PATH}\")\n",
    "\n",
    "    with open(INPUT_CLUSTER_RESULTS_PATH, 'rb') as f:\n",
    "        cluster_results = pickle.load(f)\n",
    "    clusters = cluster_results['clusters']\n",
    "    print(f\"成功加载 {len(clusters)} 个簇。\")\n",
    "\n",
    "    vectors = np.load(INPUT_VECTORS_PATH)\n",
    "    if vectors.dtype != np.float32:\n",
    "        vectors = vectors.astype(np.float32)\n",
    "    print(f\"成功加载 {len(vectors)} 个向量。\")\n",
    "\n",
    "    print(\"\\n--- 正在计算簇内相似度... ---\")\n",
    "    all_intra_cluster_similarities = []\n",
    "\n",
    "    for cluster_indices in tqdm(clusters, desc=\"[计算中]\"):\n",
    "        if len(cluster_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        cluster_vectors = vectors[cluster_indices]\n",
    "        sim_matrix = cosine_similarity(cluster_vectors)\n",
    "        upper_triangle_indices = np.triu_indices(len(cluster_indices), k=1)\n",
    "        pairwise_sims = sim_matrix[upper_triangle_indices]\n",
    "        all_intra_cluster_similarities.extend(pairwise_sims.tolist())\n",
    "\n",
    "    if not all_intra_cluster_similarities:\n",
    "        print(\"\\n错误：未能计算出任何相似度分数。请检查簇是否都过小（成员数<2）。\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- 计算完成！共得到 {len(all_intra_cluster_similarities):,} 个相似度分数。---\")\n",
    "\n",
    "    print(\"--- 正在生成相似度分布图... ---\")\n",
    "    \n",
    "    sim_array = np.array(all_intra_cluster_similarities)\n",
    "    \n",
    "    print(f\"  - 平均相似度: {np.mean(sim_array):.4f}\")\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    sns.histplot(sim_array, bins=100, kde=True, color='skyblue', alpha=0.7, ax=ax)\n",
    "    \n",
    "    if S_MIN_THRESHOLD is not None:\n",
    "        plt.axvline(x=S_MIN_THRESHOLD, color='r', linestyle='--', linewidth=2, \n",
    "                    label=f'S_MIN Threshold = {S_MIN_THRESHOLD}')\n",
    "        plt.legend(fontsize=LEGEND_FONTSIZE)\n",
    "\n",
    "    plt.xlabel('Cosine Similarity', fontsize=AXIS_LABEL_FONTSIZE) \n",
    "\n",
    "    ax.text(0.5, -0.2, '(d)', \n",
    "            transform=ax.transAxes, \n",
    "            ha='center', \n",
    "            va='top', \n",
    "            fontsize=SUB_LABEL_FONTSIZE)\n",
    "    \n",
    "    plt.ylabel('Frequency', fontsize=AXIS_LABEL_FONTSIZE)\n",
    "    \n",
    "    plt.tick_params(axis='both', which='major', labelsize=TICK_LABEL_FONTSIZE)\n",
    "\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.ylim(0, 20000)\n",
    "\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(prune='lower'))\n",
    "\n",
    "    def custom_tick_formatter(x, pos):\n",
    "        if x == 0:\n",
    "            return '0'\n",
    "        else:\n",
    "            return f'{x:.1f}'\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(custom_tick_formatter))\n",
    "\n",
    "    output_dir = os.path.dirname(OUTPUT_FIGURE_PATH)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"已创建输出目录: {output_dir}\")\n",
    "\n",
    "    plt.tight_layout(pad=1.5)\n",
    "    \n",
    "    plt.savefig(OUTPUT_FIGURE_PATH, dpi=300) \n",
    "    \n",
    "    print(f\"\\n评估完成，图像已成功保存至: {OUTPUT_FIGURE_PATH}\")\n",
    "    \n",
    "    print(\"正在显示相似度分布图...\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_cluster_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9cbfa-0c8e-43f9-b5e4-b2b1d977a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimension 64 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868e49a-1ba4-49f5-8002-62bccc38b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6121ea-9377-4637-ae84-f9733dc88026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-1/64/pca_64_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-1/64/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4a588-f0e3-4b50-8062-8e3d9498d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/64/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-1/64/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-1/64/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfea33-4a75-4cdd-ac95-e4815bff22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/64/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-1/64/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-1/64/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,                 \n",
    "            num_topics=n_topics,           \n",
    "            id2word=dictionary,            \n",
    "            random_state=42,              \n",
    "            passes=10,                     \n",
    "            workers=num_workers,          \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291be222-d55a-449a-ae49-bc96e0908714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/64/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/64/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-1/64/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-1/64/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a85df-40d6-453d-a676-0fa62a8881e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91019860-9290-48f7-ad77-30c05e6a419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "INPUT_HD_AGC_RESULTS_PATH = 'data/4-1/256/pca_256_hd_agc_results.pkl'\n",
    "\n",
    "INPUT_ORIGINAL_TEXT_PATH = 'data/paragraph.pkl'\n",
    "\n",
    "OUTPUT_DIR = 'data/4-1/256/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_CLUSTERS_CSV_PATH = os.path.join(OUTPUT_DIR, 'document_clusters_hd_agc.csv')\n",
    "\n",
    "OUTPUT_MERGED_DOCS_PATH = os.path.join(OUTPUT_DIR, 'merged_documents_with_clusters.pkl')\n",
    "\n",
    "def merge_documents_and_include_originals():\n",
    "    print(f\"--- 正在从 {INPUT_HD_AGC_RESULTS_PATH} 加载 HD-AGC 聚类结果... ---\")\n",
    "    if not os.path.exists(INPUT_HD_AGC_RESULTS_PATH):\n",
    "        raise FileNotFoundError(f\"HD-AGC 结果文件未找到，请检查路径: {INPUT_HD_AGC_RESULTS_PATH}\")\n",
    "    with open(INPUT_HD_AGC_RESULTS_PATH, 'rb') as f:\n",
    "        hd_agc_results = pickle.load(f)\n",
    "    \n",
    "    clusters = hd_agc_results.get('clusters', [])\n",
    "    \n",
    "    print(f\"--- 正在从 {INPUT_ORIGINAL_TEXT_PATH} 加载原始文本... ---\")\n",
    "    if not os.path.exists(INPUT_ORIGINAL_TEXT_PATH):\n",
    "        raise FileNotFoundError(f\"原始文本文件未找到，请检查路径: {INPUT_ORIGINAL_TEXT_PATH}\")\n",
    "    with open(INPUT_ORIGINAL_TEXT_PATH, 'rb') as f:\n",
    "        paragraph = pickle.load(f)\n",
    "        \n",
    "    n_total_docs = len(paragraph)\n",
    "\n",
    "    print(\"\\n--- HD-AGC 聚类结果分析 ---\")\n",
    "    n_clusters = len(clusters)\n",
    "    num_clustered_docs = sum(len(c) for c in clusters)\n",
    "    \n",
    "    print(f\"从 {n_total_docs} 篇文档中，识别出:\")\n",
    "    print(f\"   - {n_clusters} 个簇，共包含 {num_clustered_docs} 篇文档。\")\n",
    "    if n_clusters > 0:\n",
    "        avg_docs_per_cluster = num_clustered_docs / n_clusters\n",
    "        print(f\"   - 平均每个簇由 {avg_docs_per_cluster:.2f} 篇原始文档构成。\")\n",
    "        cluster_sizes = [len(c) for c in clusters]\n",
    "        top_10_indices = sorted(range(len(cluster_sizes)), key=lambda i: cluster_sizes[i], reverse=True)[:10]\n",
    "        \n",
    "        print(\"\\n--- Top 10 最大簇的文档数: ---\")\n",
    "        for i, cluster_idx in enumerate(top_10_indices):\n",
    "            print(f\"  - 簇 {cluster_idx} (第 {i+1} 大): {cluster_sizes[cluster_idx]} 个文档\")\n",
    "\n",
    "    print(\"\\n--- 正在构建每篇文档的聚类标签... ---\")\n",
    "    labels = [-1] * n_total_docs\n",
    "    for cluster_id, doc_indices in enumerate(clusters):\n",
    "        for doc_index in doc_indices:\n",
    "            labels[doc_index] = cluster_id\n",
    "            \n",
    "    documents_text = [\" \".join(text) for text in paragraph]\n",
    "    results_df = pd.DataFrame({\n",
    "        'document_index': range(n_total_docs),\n",
    "        'document_text': documents_text,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    results_df.sort_values(by='cluster_label', inplace=True)\n",
    "    results_df.to_csv(OUTPUT_CLUSTERS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"详细结果已保存到: {OUTPUT_CLUSTERS_CSV_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 正在构建最终文档集合... ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    merged_docs_list = []\n",
    "\n",
    "    print(f\"--- 步骤 1/2: 添加 {n_total_docs} 篇原始文档 ---\")\n",
    "    merged_docs_list.extend(paragraph)\n",
    "\n",
    "    print(f\"--- 步骤 2/2: 正在合并 {n_clusters} 个簇为超级文档 ---\")\n",
    "    for doc_indices in clusters:\n",
    "        merged_doc = []\n",
    "        for doc_index in doc_indices:\n",
    "            if 0 <= doc_index < len(paragraph):\n",
    "                merged_doc.extend(paragraph[doc_index])\n",
    "        if merged_doc:\n",
    "            merged_docs_list.append(merged_doc)\n",
    "\n",
    "    print(f\"--- 合并完成！耗时: {time.time() - start_time:.2f} 秒 ---\")\n",
    "\n",
    "    final_doc_count = len(merged_docs_list)\n",
    "    expected_doc_count = n_total_docs + n_clusters\n",
    "    print(f\"--- 共生成 {final_doc_count} 篇文档 (由 {n_total_docs} 篇原始文档 + \"\n",
    "          f\"{n_clusters} 个超级文档组成)。 ---\")\n",
    "    if final_doc_count != expected_doc_count:\n",
    "        print(f\"   - 警告: 最终文档数 ({final_doc_count}) 与预期数 ({expected_doc_count}) 不符，请检查。\")\n",
    "\n",
    "    with open(OUTPUT_MERGED_DOCS_PATH, 'wb') as f:\n",
    "        pickle.dump(merged_docs_list, f)\n",
    "        \n",
    "    print(f\"成功将文档保存到: {OUTPUT_MERGED_DOCS_PATH}\")\n",
    "\n",
    "    if merged_docs_list and clusters:\n",
    "        first_cluster_indices = clusters[0]\n",
    "        sample_cluster_size = len(first_cluster_indices)\n",
    "        print(f\"\\n示例合并文档 (来自簇 0) 由 {sample_cluster_size} 篇原始文档合并而成。\")\n",
    "        print(f\"其前20个词为: {merged_docs_list[n_total_docs][:20]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    merge_documents_and_include_originals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73647d61-3853-45d2-b4cf-1697e0775a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/256/merged_documents_with_clusters.pkl'\n",
    "FILTERED_DICTIONARY_PATH = 'data/4-1/256/final_dictionary.dict'\n",
    "FILTERED_CORPUS_PATH = 'data/4-1/256/final_corpus.pkl'\n",
    "\n",
    "NO_BELOW = 10\n",
    "NO_ABOVE = 0.35\n",
    "KEEP_N = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- 1. 加载预处理好的分词后文本 ---\")\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\"成功加载 {len(processed_texts)} 条文本。\")\n",
    "\n",
    "    print(\"\\n--- 2. 创建初始Gensim词典 (不过滤) ---\")\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    initial_vocab_size = len(dictionary)\n",
    "    print(f\"初始词典大小: {initial_vocab_size}\")\n",
    "\n",
    "    print(\"\\n--- 3. 分析将被各个过滤规则移除的词汇 ---\")\n",
    "    \n",
    "    num_docs = dictionary.num_docs\n",
    "    print(f\"总文档数: {num_docs}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    no_below_limit = NO_BELOW\n",
    "    low_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < no_below_limit}\n",
    "    print(f\"规则 'no_below = {NO_BELOW}' 分析:\")\n",
    "    print(f\" - 文档频率低于 {no_below_limit} 的词汇有 {len(low_freq_ids)} 个。\")\n",
    "\n",
    "    no_above_limit = num_docs * NO_ABOVE\n",
    "    high_freq_ids = {tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq > no_above_limit}\n",
    "    print(f\"\\n规则 'no_above = {NO_ABOVE}' 分析:\")\n",
    "    print(f\" - 文档频率高于 {NO_ABOVE:.0%} (即 > {int(no_above_limit)}个文档) 的词汇有 {len(high_freq_ids)} 个。\")\n",
    "\n",
    "    if high_freq_ids:\n",
    "        print(\" - 示例 (将被移除的高频词):\")\n",
    "        sorted_high_freq = sorted(high_freq_ids, key=lambda tokenid: dictionary.dfs[tokenid], reverse=True)\n",
    "        for tokenid in sorted_high_freq[:5]:\n",
    "            print(f\"   - '{dictionary[tokenid]}' (在 {dictionary.dfs[tokenid]} 个文档中出现)\")\n",
    "\n",
    "    total_removed_ids = low_freq_ids.union(high_freq_ids)\n",
    "    final_vocab_size_estimated = initial_vocab_size - len(total_removed_ids)\n",
    "\n",
    "    print(\"\\n--- 综合分析结果 ---\")\n",
    "    print(f\"将被移除的低频词总数: {len(low_freq_ids)}\")\n",
    "    print(f\"将被移除的高频词总数: {len(high_freq_ids)}\")\n",
    "    print(f\"将被移除的独立词汇总数: {len(total_removed_ids)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"预计过滤后的词典大小: {final_vocab_size_estimated}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n--- 4. 实际执行Gensim的 filter_extremes 操作 ---\")\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE, keep_n=KEEP_N)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    final_vocab_size_actual = len(dictionary)\n",
    "    print(f\"Gensim过滤后，实际最终词典大小: {final_vocab_size_actual}\")\n",
    "\n",
    "    if final_vocab_size_actual == final_vocab_size_estimated:\n",
    "        print(\"验证成功：手动分析结果与Gensim执行结果一致。\")\n",
    "    else:\n",
    "        print(\"警告：手动分析结果与Gensim执行结果不一致，请检查逻辑。\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 5. 创建并保存最终的BoW语料库和词典 ---\")\n",
    "\n",
    "    output_dir = os.path.dirname(FILTERED_DICTIONARY_PATH)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "    \n",
    "    dictionary.save(FILTERED_DICTIONARY_PATH)\n",
    "    with open(FILTERED_CORPUS_PATH, 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "        \n",
    "    print(f\"最终词典已保存至: {FILTERED_DICTIONARY_PATH}\")\n",
    "    print(f\"最终BoW语料库已保存至: {FILTERED_CORPUS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b30647-0670-4186-8e48-73f8a86fb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import multiprocessing\n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/256/final_dictionary.dict'\n",
    "FINAL_CORPUS_PATH = 'data/4-1/256/final_corpus.pkl'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-1/256/model/'\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count() - 2 if multiprocessing.cpu_count() > 2 else 1\n",
    "    print(f\"--- 将为每个LDA模型训练使用 {num_workers} 个CPU核心 ---\")\n",
    "\n",
    "    print(\"\\n--- 1. 正在加载经过词典过滤的最终语料库和词典... ---\")\n",
    "    if not os.path.exists(FINAL_DICTIONARY_PATH) or not os.path.exists(FINAL_CORPUS_PATH):\n",
    "        raise FileNotFoundError(\"错误：找不到最终的词典或语料库文件。请先运行词典过滤脚本。\")\n",
    "        \n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    with open(FINAL_CORPUS_PATH, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    print(f\"加载成功。词典大小: {len(dictionary)}，语料库文档数: {len(corpus)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- 2. 开始批量训练LDA模型 ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for n_topics in TOPIC_RANGE:\n",
    "        print(f\"\\n--- 开始训练: {n_topics} 主题 ---\")\n",
    "\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,                 \n",
    "            num_topics=n_topics,           \n",
    "            id2word=dictionary,            \n",
    "            random_state=42,              \n",
    "            passes=10,                     \n",
    "            workers=num_workers,          \n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'lda_model_{n_topics}.model')\n",
    "        model.save(model_path)\n",
    "        print(f\"--- 已保存: {n_topics} 主题的模型至 {model_path} ---\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- 全部模型训练完毕, 总耗时: {end_time - start_time:.2f} 秒 ---\")\n",
    "    print(f\"所有模型已保存在: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a85eb7-2b03-4bb2-9256-a6701a28d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaMulticore, KeyedVectors\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, InvertedRBO\n",
    "\n",
    "PROCESSED_CORPUS_PICKLE = 'data/4-1/256/merged_documents_with_clusters.pkl' \n",
    "\n",
    "FINAL_DICTIONARY_PATH = 'data/4-1/256/final_dictionary.dict'\n",
    "\n",
    "TENCENT_WV_PATH = 'data/origin/tencent-ailab-embedding-zh-d100-v0.2.0-s.txt'\n",
    "\n",
    "MODEL_SAVE_DIR = 'data/4-1/256/model/'\n",
    "\n",
    "RESULTS_CSV_PATH = 'data/4-1/256/lda_f_evaluation.csv'\n",
    "\n",
    "TOPIC_RANGE = range(3, 16)\n",
    "\n",
    "def evaluate_models(model_dir, topic_range, processed_texts, dictionary, word_vectors):\n",
    "    print(\"\\n--- 开始进行模型评估 ---\")\n",
    "    results = []\n",
    "\n",
    "    diversity_metric = TopicDiversity(topk=10)\n",
    "    rbo_metric = InvertedRBO(topk=10, weight=0.9)\n",
    "\n",
    "    for n_topics in topic_range:\n",
    "        model_path = os.path.join(model_dir, f'lda_model_{n_topics}.model')\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"警告: 模型文件未找到，跳过: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- 正在评估: {n_topics} 主题的模型 ---\")\n",
    "        lda_model = LdaMulticore.load(model_path)\n",
    "\n",
    "        topics_for_coherence = [[word for word, _ in lda_model.show_topic(i, topn=20)] for i in range(n_topics)]\n",
    "        topics_for_diversity = [[word for word, _ in lda_model.show_topic(i, topn=10)] for i in range(n_topics)]\n",
    "\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_for_coherence,\n",
    "            texts=processed_texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_w2v',\n",
    "            keyed_vectors=word_vectors\n",
    "        )\n",
    "        cw2v_semantic = coherence_model.get_coherence()\n",
    "\n",
    "        model_output_for_diversity = {\"topics\": topics_for_diversity}\n",
    "        diversity = diversity_metric.score(model_output_for_diversity)\n",
    "        rbo = rbo_metric.score(model_output_for_diversity)\n",
    "\n",
    "        print(f\"  - C_W2V (Semantic, topk=20): {cw2v_semantic:.4f}\")\n",
    "        print(f\"  - Topic Diversity (topk=10): {diversity:.4f}\")\n",
    "        print(f\"  - InvertedRBO (topk=10): {rbo:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"num_topics\": n_topics,\n",
    "            \"C_W2V (Semantic)\": cw2v_semantic,\n",
    "            \"Topic Diversity\": diversity,\n",
    "            \"InvertedRBO\": rbo\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results).set_index(\"num_topics\")\n",
    "\n",
    "def plot_results(results_df):\n",
    "    print(\"\\n--- 正在可视化评估结果... ---\")\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('C_W2V Semantic Coherence (topk=20)', color='tab:red')\n",
    "    ax1.plot(results_df.index, results_df['C_W2V (Semantic)'], color='tab:red', marker='o', linewidth=2.5, label='C_W2V (Semantic)')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Diversity Scores (topk=10)', color='tab:blue')\n",
    "    ax2.plot(results_df.index, results_df['Topic Diversity'], color='tab:blue', marker='x', linestyle='-', label='Topic Diversity')\n",
    "    ax2.plot(results_df.index, results_df['InvertedRBO'], color='tab:cyan', marker='x', linestyle='--', label='InvertedRBO')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.96), ncol=3, fontsize='medium')\n",
    "    fig.suptitle('LDA-F 模型评估: 语义一致性 vs. 多样性', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"--- 1. 正在加载评估所需的文件... ---\")\n",
    "\n",
    "    with open(PROCESSED_CORPUS_PICKLE, 'rb') as f:\n",
    "        processed_texts = pickle.load(f)\n",
    "    print(f\" - 成功加载 {len(processed_texts)} 条原始文本。\")\n",
    "\n",
    "    dictionary = Dictionary.load(FINAL_DICTIONARY_PATH)\n",
    "    print(f\" - 成功加载最终词典 (大小: {len(dictionary)})。\")\n",
    "\n",
    "    print(\" - 正在加载腾讯词向量模型...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(TENCENT_WV_PATH, binary=False)\n",
    "    print(\" - 成功加载腾讯词向量。\")\n",
    "\n",
    "    results_df = evaluate_models(MODEL_SAVE_DIR, TOPIC_RANGE, processed_texts, dictionary, word_vectors)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n--- 评估完成, 最终结果如下: ---\")\n",
    "        print(results_df)\n",
    "\n",
    "        os.makedirs(os.path.dirname(RESULTS_CSV_PATH), exist_ok=True)\n",
    "        results_df.to_csv(RESULTS_CSV_PATH)\n",
    "        print(f\"\\n评估结果已保存到 {RESULTS_CSV_PATH}\")\n",
    "        \n",
    "        plot_results(results_df)\n",
    "    else:\n",
    "        print(\"\\n--- 评估失败: 在指定目录下未找到任何模型文件 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
